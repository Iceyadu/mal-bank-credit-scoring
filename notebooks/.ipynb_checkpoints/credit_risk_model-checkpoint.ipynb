{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Credit Risk Modeling for Mal Bank (Sharia-Compliant)\n",
        "\n",
        "## Case Study: Credit Scoring Model Development\n",
        "\n",
        "This notebook implements a comprehensive credit risk modeling pipeline for Mal Bank, a Sharia-compliant financial institution. The project includes:\n",
        "\n",
        "1. **Credit Scoring Model Development** - Baseline and advanced models\n",
        "2. **Islamic Lending Context** - PD/EAD/LGD considerations for Murabaha, Ijara, etc.\n",
        "3. **Behavioural & Limit Management** - Early warning systems and limit recommendations\n",
        "4. **Production & Monitoring** - Architecture and monitoring strategies\n",
        "5. **Ethical & Bias Considerations** - Fairness and bias mitigation\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Credit Scoring Model Development\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('../src')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import custom modules\n",
        "from data_loading import load_all_data\n",
        "from feature_engineering import build_feature_table\n",
        "from modeling import (\n",
        "    get_preprocessing_pipeline, \n",
        "    train_logistic_regression, \n",
        "    train_lightgbm,\n",
        "    prepare_features_and_target\n",
        ")\n",
        "from evaluation import evaluate_model, plot_roc_curve, plot_pr_curve, plot_confusion_matrix, print_metrics\n",
        "from explainability import (\n",
        "    extract_logistic_coefficients, \n",
        "    plot_logistic_coefficients,\n",
        "    plot_shap_summary,\n",
        "    plot_shap_waterfall\n",
        ")\n",
        "from utils import identify_target_column, get_column_types\n",
        "\n",
        "# Set style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except:\n",
        "        plt.style.use('ggplot')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Data Loading & Initial Exploration\n",
        "\n",
        "We start by loading all data files and performing initial exploration to understand the structure and identify key columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data - adjust path if needed\n",
        "# Option 1: If data is in ../data/ directory\n",
        "# data_dir = \"../data\"\n",
        "\n",
        "# Option 2: If data is in ../malbank_case_data/ directory\n",
        "data_dir = \"../malbank_case_data\"\n",
        "\n",
        "print(\"Loading data files...\")\n",
        "data = load_all_data(data_dir)\n",
        "\n",
        "print(\"\\nData loaded successfully!\")\n",
        "print(f\"Keys: {list(data.keys())}\")\n",
        "\n",
        "# Extract individual tables\n",
        "apps = data['applications']\n",
        "prev_apps = data['previous_applications']\n",
        "inst = data['installments']\n",
        "bureau = data['bureau']\n",
        "bureau_bal = data['bureau_balance']\n",
        "cc_bal = data['credit_card_balance']\n",
        "pos_bal = data['pos_cash_balance']\n",
        "cols_desc = data['columns_description']\n",
        "\n",
        "print(f\"\\nApplications shape: {apps.shape}\")\n",
        "print(f\"Previous applications shape: {prev_apps.shape}\")\n",
        "print(f\"Installments shape: {inst.shape}\")\n",
        "print(f\"Bureau shape: {bureau.shape}\")\n",
        "print(f\"Bureau balance shape: {bureau_bal.shape}\")\n",
        "print(f\"Credit card balance shape: {cc_bal.shape}\")\n",
        "print(f\"POS cash balance shape: {pos_bal.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore applications table\n",
        "print(\"Applications table info:\")\n",
        "print(apps.info())\n",
        "print(\"\\nFirst few rows:\")\n",
        "apps.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify target column\n",
        "target_col = identify_target_column(apps)\n",
        "print(f\"Target column identified: {target_col}\")\n",
        "\n",
        "if target_col:\n",
        "    print(f\"\\nTarget distribution:\")\n",
        "    print(apps[target_col].value_counts())\n",
        "    print(f\"\\nTarget proportion:\")\n",
        "    print(apps[target_col].value_counts(normalize=True))\n",
        "    \n",
        "    # Visualize target distribution\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    apps[target_col].value_counts().plot(kind='bar', color=['skyblue', 'coral'])\n",
        "    plt.title('Target Distribution', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Target (0=No Default, 1=Default)', fontsize=12)\n",
        "    plt.ylabel('Count', fontsize=12)\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.grid(alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Feature Engineering - Aggregating Supporting Tables\n",
        "\n",
        "We aggregate features from supporting tables (previous applications, installments, bureau, etc.) to create a comprehensive feature set at the application level.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build feature table by aggregating all supporting tables\n",
        "print(\"Building feature table...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "feature_df = build_feature_table(\n",
        "    apps=apps,\n",
        "    prev_apps=prev_apps,\n",
        "    inst=inst,\n",
        "    bureau=bureau,\n",
        "    bureau_bal=bureau_bal,\n",
        "    cc_bal=cc_bal,\n",
        "    pos_bal=pos_bal\n",
        ")\n",
        "\n",
        "print(f\"\\nFeature table shape: {feature_df.shape}\")\n",
        "print(f\"Original applications shape: {apps.shape}\")\n",
        "print(f\"New features added: {feature_df.shape[1] - apps.shape[1]}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values in aggregated features:\")\n",
        "agg_cols = [col for col in feature_df.columns if col not in apps.columns]\n",
        "missing_counts = feature_df[agg_cols].isnull().sum()\n",
        "print(missing_counts[missing_counts > 0].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Target and Basic Cleaning\n",
        "\n",
        "We identify the target, handle missing data, cap outliers, and prepare for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X, y = prepare_features_and_target(feature_df, target_col=target_col)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(y.value_counts())\n",
        "\n",
        "# Identify column types\n",
        "categorical_cols, numerical_cols = get_column_types(X)\n",
        "print(f\"\\nCategorical columns: {len(categorical_cols)}\")\n",
        "print(f\"Numerical columns: {len(numerical_cols)}\")\n",
        "print(f\"\\nFirst 10 categorical: {categorical_cols[:10]}\")\n",
        "print(f\"First 10 numerical: {numerical_cols[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining target distribution:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(f\"\\nTest target distribution:\")\n",
        "print(y_test.value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Model Training\n",
        "\n",
        "We train two models:\n",
        "1. **Logistic Regression** - Interpretable baseline model\n",
        "2. **LightGBM** - Non-linear gradient boosting model\n",
        "\n",
        "Both models use the same preprocessing pipeline with proper handling of missing values, outliers, and class imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build preprocessing pipeline\n",
        "print(\"Building preprocessing pipeline...\")\n",
        "preprocessing = get_preprocessing_pipeline(\n",
        "    X_train,\n",
        "    categorical_cols=categorical_cols,\n",
        "    numerical_cols=numerical_cols,\n",
        "    cap_outliers=True\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Logistic Regression\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model_lr = train_logistic_regression(\n",
        "    X_train, \n",
        "    y_train, \n",
        "    preprocessing,\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Logistic Regression trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate scale_pos_weight for LightGBM\n",
        "neg_count = (y_train == 0).sum()\n",
        "pos_count = (y_train == 1).sum()\n",
        "scale_pos_weight = neg_count / (pos_count + 1e-6)\n",
        "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "# Train LightGBM\n",
        "print(\"Training LightGBM model...\")\n",
        "model_lgb = train_lightgbm(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    preprocessing,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7\n",
        ")\n",
        "\n",
        "print(\"LightGBM trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Model Evaluation\n",
        "\n",
        "We evaluate both models using comprehensive metrics: AUC-ROC, AUC-PR, KS statistic, Brier score, and confusion matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Logistic Regression\n",
        "print(\"Evaluating Logistic Regression...\")\n",
        "metrics_lr = evaluate_model(model_lr, X_test, y_test, threshold=0.5, model_name=\"Logistic Regression\")\n",
        "print_metrics(metrics_lr, \"Logistic Regression\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate LightGBM\n",
        "print(\"Evaluating LightGBM...\")\n",
        "metrics_lgb = evaluate_model(model_lgb, X_test, y_test, threshold=0.5, model_name=\"LightGBM\")\n",
        "print_metrics(metrics_lgb, \"LightGBM\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC curves\n",
        "plot_roc_curve(\n",
        "    y_test.values, \n",
        "    metrics_lr['y_pred_proba'], \n",
        "    \"Logistic Regression\",\n",
        "    save_path=\"../plots/roc_logreg.png\"\n",
        ")\n",
        "\n",
        "plot_roc_curve(\n",
        "    y_test.values, \n",
        "    metrics_lgb['y_pred_proba'], \n",
        "    \"LightGBM\",\n",
        "    save_path=\"../plots/roc_lgbm.png\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Precision-Recall curves\n",
        "plot_pr_curve(\n",
        "    y_test.values,\n",
        "    metrics_lr['y_pred_proba'],\n",
        "    \"Logistic Regression\",\n",
        "    save_path=\"../plots/prc_logreg.png\"\n",
        ")\n",
        "\n",
        "plot_pr_curve(\n",
        "    y_test.values,\n",
        "    metrics_lgb['y_pred_proba'],\n",
        "    \"LightGBM\",\n",
        "    save_path=\"../plots/prc_lgbm.png\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "plot_confusion_matrix(\n",
        "    metrics_lr['confusion_matrix'],\n",
        "    \"Logistic Regression\",\n",
        "    threshold=0.5,\n",
        "    save_path=\"../plots/confusion_logreg.png\"\n",
        ")\n",
        "\n",
        "plot_confusion_matrix(\n",
        "    metrics_lgb['confusion_matrix'],\n",
        "    \"LightGBM\",\n",
        "    threshold=0.5,\n",
        "    save_path=\"../plots/confusion_lgbm.png\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6 Model Explainability\n",
        "\n",
        "We analyze model interpretability using:\n",
        "- **Logistic Regression**: Coefficient analysis\n",
        "- **LightGBM**: SHAP values for global and local explanations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and plot Logistic Regression coefficients\n",
        "top_positive_lr, top_negative_lr = extract_logistic_coefficients(\n",
        "    model_lr,\n",
        "    feature_names=list(X_train.columns),\n",
        "    top_n=15\n",
        ")\n",
        "\n",
        "print(\"Top 15 Positive Coefficients (increase default risk):\")\n",
        "print(top_positive_lr)\n",
        "\n",
        "print(\"\\nTop 15 Negative Coefficients (decrease default risk):\")\n",
        "print(top_negative_lr)\n",
        "\n",
        "plot_logistic_coefficients(\n",
        "    top_positive_lr,\n",
        "    top_negative_lr,\n",
        "    model_name=\"Logistic Regression\",\n",
        "    save_path=\"../plots/coefficients_logreg.png\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP summary plot for LightGBM\n",
        "print(\"Computing SHAP values for LightGBM (this may take a few minutes)...\")\n",
        "plot_shap_summary(\n",
        "    model_lgb,\n",
        "    X_test.sample(min(500, len(X_test)), random_state=42),\n",
        "    model_name=\"LightGBM\",\n",
        "    save_path=\"../plots/shap_summary_lgbm.png\",\n",
        "    n_samples=100\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find examples for local explanation\n",
        "# High-risk customer (predicted default with high probability)\n",
        "high_risk_idx = np.where((metrics_lgb['y_pred_proba'] > 0.7) & (y_test.values == 1))[0]\n",
        "if len(high_risk_idx) > 0:\n",
        "    high_risk_idx = high_risk_idx[0]\n",
        "    print(f\"High-risk customer (actual default): Index {high_risk_idx}, Probability: {metrics_lgb['y_pred_proba'][high_risk_idx]:.3f}\")\n",
        "else:\n",
        "    # Use highest probability default prediction\n",
        "    high_risk_idx = np.argmax(metrics_lgb['y_pred_proba'])\n",
        "    print(f\"High-risk customer (highest predicted probability): Index {high_risk_idx}, Probability: {metrics_lgb['y_pred_proba'][high_risk_idx]:.3f}\")\n",
        "\n",
        "# Low-risk customer (predicted non-default with low probability)\n",
        "low_risk_idx = np.where((metrics_lgb['y_pred_proba'] < 0.2) & (y_test.values == 0))[0]\n",
        "if len(low_risk_idx) > 0:\n",
        "    low_risk_idx = low_risk_idx[0]\n",
        "    print(f\"Low-risk customer (actual non-default): Index {low_risk_idx}, Probability: {metrics_lgb['y_pred_proba'][low_risk_idx]:.3f}\")\n",
        "else:\n",
        "    # Use lowest probability\n",
        "    low_risk_idx = np.argmin(metrics_lgb['y_pred_proba'])\n",
        "    print(f\"Low-risk customer (lowest predicted probability): Index {low_risk_idx}, Probability: {metrics_lgb['y_pred_proba'][low_risk_idx]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP waterfall plots for individual customers\n",
        "print(\"\\nGenerating SHAP waterfall for high-risk customer...\")\n",
        "plot_shap_waterfall(\n",
        "    model_lgb,\n",
        "    X_test.reset_index(drop=True),\n",
        "    high_risk_idx,\n",
        "    model_name=\"LightGBM\",\n",
        "    save_path=\"../plots/shap_waterfall_high_risk.png\"\n",
        ")\n",
        "\n",
        "print(\"\\nGenerating SHAP waterfall for low-risk customer...\")\n",
        "plot_shap_waterfall(\n",
        "    model_lgb,\n",
        "    X_test.reset_index(drop=True),\n",
        "    low_risk_idx,\n",
        "    model_name=\"LightGBM\",\n",
        "    save_path=\"../plots/shap_waterfall_low_risk.png\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Local Explanation: High-Risk Customer\n",
        "\n",
        "**Customer Profile Analysis:**\n",
        "\n",
        "Based on the SHAP waterfall plot above, this customer shows high default risk due to:\n",
        "\n",
        "- **Key Risk Factors**: [Analyze top positive SHAP values from the plot]\n",
        "  - High utilization of credit facilities\n",
        "  - History of late payments (DPD > 0)\n",
        "  - Low income relative to credit amount\n",
        "  - Previous application rejections or cancellations\n",
        "\n",
        "**Credit Officer Action:**\n",
        "\n",
        "1. **Immediate Review**: Flag for manual underwriting review\n",
        "2. **Additional Documentation**: Request proof of income stability, employment verification\n",
        "3. **Risk Mitigation**: Consider:\n",
        "   - Lower credit limit\n",
        "   - Shorter repayment term\n",
        "   - Require co-signer or collateral (for Murabaha/Ijara)\n",
        "   - Stricter monitoring of payment behavior\n",
        "\n",
        "**Sharia-Compliant Considerations:**\n",
        "\n",
        "- For Murabaha: Ensure asset valuation is conservative\n",
        "- For Ijara: Verify lessee's ability to maintain regular payments\n",
        "- No interest-based penalties; focus on restructuring if needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Local Explanation: Low-Risk Customer\n",
        "\n",
        "**Customer Profile Analysis:**\n",
        "\n",
        "Based on the SHAP waterfall plot above, this customer shows low default risk due to:\n",
        "\n",
        "- **Key Positive Factors**: [Analyze top negative SHAP values from the plot]\n",
        "  - Strong payment history (no late payments)\n",
        "  - Low credit utilization\n",
        "  - Stable income and employment\n",
        "  - Good bureau credit history\n",
        "  - Previous successful loan completions\n",
        "\n",
        "**Credit Officer Action:**\n",
        "\n",
        "1. **Standard Processing**: Approve with standard terms\n",
        "2. **Potential Upsell**: Consider offering:\n",
        "   - Higher credit limit (if utilization is low)\n",
        "   - Additional Sharia-compliant products (Murabaha, Ijara)\n",
        "   - Preferred customer benefits\n",
        "\n",
        "**Sharia-Compliant Considerations:**\n",
        "\n",
        "- Good candidates for profit-sharing products (Mudarabah/Musharakah) if applicable\n",
        "- Can support higher-value asset financing (Murabaha)\n",
        "- Eligible for longer-term Ijara contracts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Islamic Lending Context\n",
        "\n",
        "### 2.1 Differences Between Conventional and Islamic Lending\n",
        "\n",
        "**Fundamental Principles:**\n",
        "\n",
        "1. **Prohibition of Riba (Interest)**: Islamic finance prohibits charging or paying interest. Instead, transactions must be asset-backed and involve profit-sharing or cost-plus structures.\n",
        "\n",
        "2. **Asset-Backed Financing**: All financing must be tied to real assets or services, ensuring tangible value creation.\n",
        "\n",
        "3. **Risk-Sharing**: Islamic finance emphasizes sharing both profits and losses between the bank and customer, rather than fixed interest payments.\n",
        "\n",
        "**Key Islamic Financing Products:**\n",
        "\n",
        "- **Murabaha**: Cost-plus sale where the bank purchases an asset and sells it to the customer at a marked-up price, payable in installments.\n",
        "- **Ijara**: Leasing arrangement where the bank owns the asset and leases it to the customer for regular payments.\n",
        "- **Mudarabah**: Profit-sharing partnership where the bank provides capital and the customer provides expertise.\n",
        "- **Musharakah**: Joint venture partnership with shared profits and losses.\n",
        "\n",
        "### 2.2 Impact on Credit Risk Modeling\n",
        "\n",
        "**Feature Engineering Adjustments:**\n",
        "\n",
        "- **No Interest-Based Features**: Remove interest rate, APR, or interest payment history\n",
        "- **Asset Valuation**: Include features for asset value, depreciation, and market conditions\n",
        "- **Profit Payment History**: Track deferred profit payments (equivalent to interest in conventional) but structured differently\n",
        "- **Installment Structure**: Focus on principal + profit components separately\n",
        "\n",
        "**Target Definition:**\n",
        "\n",
        "- **Default**: Failure to meet agreed installment/profit payments (not unpaid interest)\n",
        "- **Delinquency**: Days past due on installments (principal + profit), not interest arrears\n",
        "- **Loss**: Outstanding principal + unearned profit at default, minus asset recovery value\n",
        "\n",
        "### 2.3 PD, EAD, and LGD in Islamic Context\n",
        "\n",
        "**Probability of Default (PD):**\n",
        "\n",
        "- **Definition**: Probability that a customer fails to meet agreed installment/profit payments\n",
        "- **Modeling**: Similar to conventional, but payment obligations are structured as:\n",
        "  - Principal repayment (return of capital)\n",
        "  - Profit margin (deferred profit in Murabaha, rental in Ijara)\n",
        "- **Features**: Payment history, income stability, asset value trends, economic conditions\n",
        "\n",
        "**Exposure at Default (EAD):**\n",
        "\n",
        "- **Murabaha**: \n",
        "  - Outstanding principal (remaining cost)\n",
        "  - Unearned profit (profit not yet realized)\n",
        "  - Formula: `EAD = Outstanding Principal + (Total Profit × Remaining Installments / Total Installments)`\n",
        "  \n",
        "- **Ijara**:\n",
        "  - Remaining lease payments (rental + principal if applicable)\n",
        "  - Residual asset value (if customer defaults, bank retains asset)\n",
        "  - Formula: `EAD = Sum of Remaining Lease Payments - Expected Residual Value`\n",
        "\n",
        "- **Mudarabah/Musharakah**:\n",
        "  - Outstanding capital contribution\n",
        "  - Expected profit share (if applicable)\n",
        "\n",
        "**Loss Given Default (LGD):**\n",
        "\n",
        "- **Components**:\n",
        "  1. **Outstanding Amount**: EAD at default\n",
        "  2. **Recovery Value**: \n",
        "     - Asset sale proceeds (Murabaha: sell asset; Ijara: re-lease or sell)\n",
        "     - Collateral liquidation (if applicable)\n",
        "  3. **Recovery Costs**: Legal, administrative, asset disposal costs\n",
        "  \n",
        "- **Formula**: `LGD = 1 - (Recovery Value - Recovery Costs) / EAD`\n",
        "\n",
        "- **Sharia Considerations**:\n",
        "  - No penalty interest on overdue amounts\n",
        "  - Focus on asset repossession and resale\n",
        "  - Restructuring options (rescheduling, resale) preferred over foreclosure\n",
        "\n",
        "### 2.4 Murabaha-Specific Modeling\n",
        "\n",
        "**Delinquency Measurement:**\n",
        "\n",
        "- **Days Past Due (DPD)**: Days since last successful installment payment\n",
        "- **Installment Components**:\n",
        "  - Principal component (return of capital)\n",
        "  - Profit component (deferred profit margin)\n",
        "- **Tracking**: Monitor both components separately, but DPD applies to total installment\n",
        "\n",
        "**Loss Severity Calculation:**\n",
        "\n",
        "1. **At Default**:\n",
        "   - Outstanding Principal = Original Cost - Principal Paid\n",
        "   - Unearned Profit = Total Profit × (Remaining Installments / Total Installments)\n",
        "   - EAD = Outstanding Principal + Unearned Profit\n",
        "\n",
        "2. **Recovery Process**:\n",
        "   - Repossess asset (if customer defaults)\n",
        "   - Sell asset at market value (may be depreciated)\n",
        "   - Recovery Value = Sale Price - Selling Costs\n",
        "\n",
        "3. **LGD Calculation**:\n",
        "   ```\n",
        "   LGD = (EAD - Recovery Value) / EAD\n",
        "   ```\n",
        "\n",
        "**Key Risk Factors for Murabaha:**\n",
        "\n",
        "- Asset depreciation rate (affects recovery value)\n",
        "- Market liquidity for asset resale\n",
        "- Customer's payment behavior on profit component\n",
        "- Economic conditions affecting asset values\n",
        "- Asset condition at default (wear and tear)\n",
        "\n",
        "**Modeling Recommendations:**\n",
        "\n",
        "- Include asset type and depreciation features\n",
        "- Track profit payment separately from principal\n",
        "- Model recovery rates by asset category\n",
        "- Consider market conditions in LGD models\n",
        "- Use conservative asset valuations for EAD calculations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Behavioural & Limit Management\n",
        "\n",
        "### 3.1 Behavioural Variables from Transactional History\n",
        "\n",
        "**DPD Trends:**\n",
        "\n",
        "- **Current DPD**: Days past due at current time\n",
        "- **DPD Buckets**: 0, 1-30, 31-60, 61-90, 90+\n",
        "- **DPD Trend**: Direction and magnitude of DPD changes over time\n",
        "- **DPD Volatility**: Standard deviation of DPD over rolling windows\n",
        "\n",
        "**Payment Behavior:**\n",
        "\n",
        "- **Missed Payments**: Count of missed payments in rolling windows (3, 6, 12 months)\n",
        "- **Late Payments**: Count of payments with DPD > 0 in rolling windows\n",
        "- **Payment-to-Scheduled Ratio**: Actual payment / scheduled payment\n",
        "- **Payment Patterns**:\n",
        "  - Full payment rate\n",
        "  - Partial payment rate\n",
        "  - Above-minimum payment rate\n",
        "  - Payment timing consistency\n",
        "\n",
        "**Utilization Metrics:**\n",
        "\n",
        "- **Current Utilization**: Current balance / Credit limit\n",
        "- **Peak Utilization**: Maximum utilization in last N months\n",
        "- **Average Utilization**: Mean utilization over rolling windows\n",
        "- **Utilization Trend**: Increasing, stable, or decreasing\n",
        "- **Utilization Volatility**: Standard deviation of utilization\n",
        "\n",
        "**Income Stability (Hypothetical):**\n",
        "\n",
        "- **Income Trend**: Increasing, stable, or decreasing\n",
        "- **Income Volatility**: Coefficient of variation of income\n",
        "- **Income-to-Payment Ratio**: Monthly income / Monthly payment obligation\n",
        "- **Payment Coverage**: Ability to cover payments from income\n",
        "\n",
        "### 3.2 Early Warning System for Delinquency\n",
        "\n",
        "**Approach 1: Behavioural Score Model**\n",
        "\n",
        "A small predictive model (e.g., logistic regression or simple tree) predicting 3-6 month delinquency:\n",
        "\n",
        "```python\n",
        "# Pseudo-code for behavioural score\n",
        "def behavioral_score_model(features):\n",
        "    \"\"\"\n",
        "    Predicts probability of delinquency in next 3-6 months.\n",
        "    \n",
        "    Features:\n",
        "    - DPD trend (increasing = risk)\n",
        "    - Payment ratio (declining = risk)\n",
        "    - Utilization (high = risk)\n",
        "    - Missed payments count (increasing = risk)\n",
        "    - Income stability indicators\n",
        "    \"\"\"\n",
        "    risk_score = model.predict_proba(features)[:, 1]\n",
        "    return risk_score\n",
        "```\n",
        "\n",
        "**Approach 2: Rule-Based Early Warning System**\n",
        "\n",
        "Define thresholds that trigger alerts:\n",
        "\n",
        "- **Yellow Alert** (Monitor):\n",
        "  - DPD: 1-15 days\n",
        "  - Payment ratio: 0.8-0.95\n",
        "  - Utilization: 80-90%\n",
        "  - 1-2 missed payments in last 6 months\n",
        "\n",
        "- **Orange Alert** (Intervene):\n",
        "  - DPD: 16-30 days\n",
        "  - Payment ratio: 0.6-0.8\n",
        "  - Utilization: 90-95%\n",
        "  - 3-4 missed payments in last 6 months\n",
        "  - Declining income trend\n",
        "\n",
        "- **Red Alert** (Immediate Action):\n",
        "  - DPD: 31+ days\n",
        "  - Payment ratio: < 0.6\n",
        "  - Utilization: > 95%\n",
        "  - 5+ missed payments in last 6 months\n",
        "  - Significant income decline\n",
        "\n",
        "**Intervention Strategies (Sharia-Compliant):**\n",
        "\n",
        "1. **Early Contact**: Reach out to customer to understand situation\n",
        "2. **Restructuring**: Reschedule installments (extend term, reduce payment amount)\n",
        "3. **Resale (Murabaha)**: If customer cannot continue, facilitate asset resale\n",
        "4. **Limit Adjustment**: Reduce or freeze credit limits\n",
        "5. **No Penalties**: Avoid interest-based penalties; focus on solutions\n",
        "\n",
        "### 3.3 Limit Management Framework\n",
        "\n",
        "**Limit Increase Conditions:**\n",
        "\n",
        "- **Good Behavioural Score**: Low risk of delinquency\n",
        "- **Low DPD**: No or minimal days past due\n",
        "- **Stable Payment History**: Consistent on-time payments\n",
        "- **Low Utilization**: Current utilization < 60%\n",
        "- **Stable/Increasing Income**: Positive income trend\n",
        "- **Long Relationship**: Successful history with bank\n",
        "\n",
        "**Limit Decrease or Freeze Conditions:**\n",
        "\n",
        "- **High DPD**: 30+ days past due\n",
        "- **Frequent Late Payments**: Multiple late payments in recent months\n",
        "- **High Utilization**: Utilization > 90%\n",
        "- **Volatile/Decreasing Income**: Negative income trend\n",
        "- **Missed Payments**: Multiple missed payments\n",
        "- **Negative Bureau Updates**: New adverse credit bureau information\n",
        "\n",
        "**Sharia-Compliant Levers:**\n",
        "\n",
        "- **No Interest-Based Penalties**: Cannot charge interest on overdue amounts\n",
        "- **Limit Adjustments**: Reduce available credit to prevent further exposure\n",
        "- **Restructuring**: Modify payment terms (extend term, reduce amount)\n",
        "- **Asset Repossession**: For Murabaha/Ijara, repossess asset if default occurs\n",
        "- **Early Settlement Incentives**: Offer discounts for early full payment (discount on profit, not interest)\n",
        "\n",
        "### 3.4 Evaluation Strategy\n",
        "\n",
        "**Backtesting Framework:**\n",
        "\n",
        "1. **Historical Data Analysis**:\n",
        "   - Apply behavioural score to historical data\n",
        "   - Identify customers who would have triggered alerts\n",
        "   - Compare outcomes: Did they default? How many were false positives?\n",
        "\n",
        "2. **Metrics**:\n",
        "   - **Bad Rate Reduction**: % reduction in default rate among intervened customers\n",
        "   - **Losses Avoided**: Estimated losses prevented through early intervention\n",
        "   - **Retention Rate**: % of customers retained after intervention\n",
        "   - **False Positive Rate**: % of alerts that did not lead to default\n",
        "   - **Cost-Benefit**: Cost of intervention vs. losses avoided\n",
        "\n",
        "3. **A/B Testing** (if possible):\n",
        "   - Randomize customers into intervention vs. control groups\n",
        "   - Measure impact of interventions on default rates\n",
        "\n",
        "**Implementation Pseudo-code:**\n",
        "\n",
        "```python\n",
        "def evaluate_limit_strategy(historical_data, model, thresholds):\n",
        "    \"\"\"\n",
        "    Backtest limit management strategy.\n",
        "    \"\"\"\n",
        "    # Apply behavioural score\n",
        "    scores = model.predict_proba(historical_data)[:, 1]\n",
        "    \n",
        "    # Identify alerts\n",
        "    alerts = apply_thresholds(scores, thresholds)\n",
        "    \n",
        "    # Compare outcomes\n",
        "    intervened_outcomes = historical_data[alerts]['default']\n",
        "    control_outcomes = historical_data[~alerts]['default']\n",
        "    \n",
        "    # Calculate metrics\n",
        "    bad_rate_reduction = (\n",
        "        control_outcomes.mean() - intervened_outcomes.mean()\n",
        "    ) / control_outcomes.mean()\n",
        "    \n",
        "    return {\n",
        "        'bad_rate_reduction': bad_rate_reduction,\n",
        "        'losses_avoided': calculate_losses_avoided(...),\n",
        "        'retention_rate': calculate_retention(...)\n",
        "    }\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Production & Monitoring\n",
        "\n",
        "### 4.1 Production Architecture\n",
        "\n",
        "**Batch Scoring (Daily Portfolio Scoring):**\n",
        "\n",
        "```\n",
        "┌─────────────────┐\n",
        "│  Data Sources   │\n",
        "│  (Applications, │\n",
        "│   Payments,     │\n",
        "│   Bureau)       │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│ Feature Store   │\n",
        "│ (Historical +   │\n",
        "│  Real-time)     │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│ Feature Prep    │\n",
        "│ Pipeline        │\n",
        "│ (Same as train) │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│ Model Artifact  │\n",
        "│ (LightGBM/      │\n",
        "│  Logistic Reg)  │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│ Scoring Engine  │\n",
        "│ (Batch Job)     │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│ Decision Rules  │\n",
        "│ (Approve/       │\n",
        "│  Reject/        │\n",
        "│  Manual Review) │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│ Results Store   │\n",
        "│ (Scores,        │\n",
        "│  Decisions)     │\n",
        "└─────────────────┘\n",
        "```\n",
        "\n",
        "**Real-Time Scoring (API):**\n",
        "\n",
        "```\n",
        "Client Request → API Gateway → Feature Extraction → \n",
        "Model Scoring → Decision Engine → Response\n",
        "```\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "1. **Feature Preparation Pipeline**: \n",
        "   - Reuse same preprocessing as training (imputation, encoding, scaling)\n",
        "   - Aggregate supporting tables in real-time or from feature store\n",
        "   - Ensure consistency with training data\n",
        "\n",
        "2. **Model Artifact Loading**:\n",
        "   - Store trained models in model registry (MLflow, S3, etc.)\n",
        "   - Version control for model artifacts\n",
        "   - Load latest approved model version\n",
        "\n",
        "3. **Scoring Endpoint**:\n",
        "   - REST API (Flask/FastAPI) for real-time scoring\n",
        "   - Batch job (Airflow, cron) for daily portfolio scoring\n",
        "   - Return: probability, risk score, decision recommendation\n",
        "\n",
        "4. **Decision Rules**:\n",
        "   - Automated: Approve (score < threshold_low), Reject (score > threshold_high)\n",
        "   - Manual Review: threshold_low < score < threshold_high\n",
        "   - Policy rules: Override based on business rules (e.g., minimum income)\n",
        "\n",
        "5. **Logging & Audit Trail**:\n",
        "   - Log all predictions with timestamps, features, scores, decisions\n",
        "   - Store in database for compliance and monitoring\n",
        "   - Enable model explainability queries (SHAP values on demand)\n",
        "\n",
        "### 4.2 Monitoring & Retraining\n",
        "\n",
        "**Data Drift Detection:**\n",
        "\n",
        "- **Population Stability Index (PSI)**:\n",
        "  - Compare feature distributions between training and production\n",
        "  - Threshold: PSI > 0.25 indicates significant drift\n",
        "  - Monitor key features: income, credit amount, utilization, DPD\n",
        "\n",
        "- **Feature Drift Metrics**:\n",
        "  ```python\n",
        "  def calculate_psi(expected, actual, bins=10):\n",
        "      \"\"\"Calculate Population Stability Index.\"\"\"\n",
        "      # Bin data\n",
        "      # Compare distributions\n",
        "      # Return PSI value\n",
        "  ```\n",
        "\n",
        "- **Action**: If PSI > threshold, investigate data quality, feature engineering, or retrain model\n",
        "\n",
        "**Performance Monitoring:**\n",
        "\n",
        "- **AUC-ROC**: Track over time (weekly/monthly)\n",
        "  - Threshold: AUC drops > 0.05 from baseline → investigate\n",
        "- **KS Statistic**: Monitor discrimination power\n",
        "  - Threshold: KS drops > 0.05 → investigate\n",
        "- **Brier Score**: Monitor calibration\n",
        "  - Threshold: Brier increases > 0.05 → investigate\n",
        "- **Precision/Recall**: Track at chosen threshold\n",
        "  - Monitor false positive/negative rates\n",
        "\n",
        "**Fairness Monitoring:**\n",
        "\n",
        "- **Group-Wise Metrics**: If protected attributes available (gender, age group, region):\n",
        "  - Calculate AUC, precision, recall by group\n",
        "  - Monitor for significant disparities\n",
        "  - Threshold: Difference > 0.05 in AUC between groups → investigate\n",
        "\n",
        "- **Proxy Variable Detection**:\n",
        "  - Identify features highly correlated with protected attributes\n",
        "  - Monitor impact on fairness metrics\n",
        "  - Consider removal or regularization if bias detected\n",
        "\n",
        "**Retraining Triggers:**\n",
        "\n",
        "1. **Performance Degradation**: AUC/KS drops below threshold\n",
        "2. **Data Drift**: PSI > 0.25 for multiple key features\n",
        "3. **Time-Based**: Quarterly or semi-annual retraining\n",
        "4. **Significant Events**: Economic shocks, regulatory changes, product changes\n",
        "5. **Fairness Issues**: Significant bias detected across groups\n",
        "\n",
        "**Retraining Process:**\n",
        "\n",
        "1. Collect new training data (last 12-24 months)\n",
        "2. Re-run feature engineering pipeline\n",
        "3. Train new model version\n",
        "4. Validate on holdout set\n",
        "5. Compare with current model (A/B test if possible)\n",
        "6. Deploy if performance improved or maintained\n",
        "7. Monitor post-deployment performance\n",
        "\n",
        "**Model Versioning:**\n",
        "\n",
        "- Use MLflow or similar for model versioning\n",
        "- Track: training date, data version, hyperparameters, performance metrics\n",
        "- Enable rollback to previous version if issues arise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: Ethical & Bias Considerations\n",
        "\n",
        "### 5.1 Ensuring Non-Discriminatory Models\n",
        "\n",
        "**Protected Attributes Exclusion:**\n",
        "\n",
        "- **Explicit Exclusion**: Remove protected attributes from features:\n",
        "  - Gender, race, religion, ethnicity, age (if protected)\n",
        "  - National origin, marital status (if protected by regulation)\n",
        "  \n",
        "- **Implementation**:\n",
        "  ```python\n",
        "  protected_attributes = ['CODE_GENDER', 'AGE', 'RELIGION', ...]\n",
        "  X_model = X.drop(columns=protected_attributes)\n",
        "  ```\n",
        "\n",
        "**Proxy Variable Detection:**\n",
        "\n",
        "- **Identification**: Features highly correlated with protected attributes may act as proxies:\n",
        "  - Example: ZIP code → race/ethnicity\n",
        "  - Example: Occupation type → gender\n",
        "  - Example: Education level → socioeconomic status (may correlate with protected attributes)\n",
        "\n",
        "- **Detection Method**:\n",
        "  1. Calculate correlation between features and protected attributes\n",
        "  2. Flag features with correlation > threshold (e.g., 0.3)\n",
        "  3. Evaluate impact on group-wise performance\n",
        "  4. Consider removal or regularization if bias detected\n",
        "\n",
        "**Group-Wise Evaluation:**\n",
        "\n",
        "- **Metrics by Group**: Calculate performance metrics separately for each protected group:\n",
        "  - AUC-ROC by gender, age group, region\n",
        "  - Precision, recall, F1 by group\n",
        "  - Default rates by group\n",
        "\n",
        "- **Disparity Detection**:\n",
        "  - Compare metrics across groups\n",
        "  - Flag if difference > threshold (e.g., 0.05 in AUC)\n",
        "  - Investigate root causes\n",
        "\n",
        "### 5.2 Handling Correlated Features\n",
        "\n",
        "**Options for Features Correlated with Protected Attributes:**\n",
        "\n",
        "1. **Removal**: \n",
        "   - Remove features with high correlation to protected attributes\n",
        "   - Pros: Simple, eliminates direct proxy\n",
        "   - Cons: May lose predictive power\n",
        "\n",
        "2. **Regularization**:\n",
        "   - Use L1/L2 regularization to reduce feature importance\n",
        "   - Pros: Retains some predictive power\n",
        "   - Cons: May not fully eliminate bias\n",
        "\n",
        "3. **Reweighting**:\n",
        "   - Adjust sample weights to balance representation\n",
        "   - Pros: Can improve fairness\n",
        "   - Cons: May reduce overall accuracy\n",
        "\n",
        "4. **Post-Processing**:\n",
        "   - Adjust thresholds by group to equalize outcomes\n",
        "   - Pros: Can achieve equalized odds or demographic parity\n",
        "   - Cons: May reduce accuracy, requires careful calibration\n",
        "\n",
        "5. **Fairness-Aware Algorithms**:\n",
        "   - Use algorithms that explicitly optimize for fairness\n",
        "   - Examples: Fairness constraints in optimization, adversarial debiasing\n",
        "   - Pros: Built-in fairness\n",
        "   - Cons: More complex, may reduce accuracy\n",
        "\n",
        "**Recommendation for Mal Bank:**\n",
        "\n",
        "- **Primary**: Remove protected attributes and high-correlation proxies\n",
        "- **Secondary**: Use regularization to reduce impact of remaining correlated features\n",
        "- **Tertiary**: Monitor group-wise metrics and apply post-processing if needed\n",
        "- **Governance**: Review by model risk committee before deployment\n",
        "\n",
        "### 5.3 Balancing Fairness, Accuracy, and Business Impact\n",
        "\n",
        "**Trade-Offs:**\n",
        "\n",
        "- **Fairness vs. Accuracy**:\n",
        "  - Removing biased features may reduce AUC by 0.01-0.03\n",
        "  - Post-processing for fairness may reduce precision/recall\n",
        "  - **Decision**: Accept small accuracy loss (e.g., < 0.02 AUC) for significant fairness gain\n",
        "\n",
        "- **Fairness vs. Business Impact**:\n",
        "  - Equalizing approval rates may increase default rate\n",
        "  - Equalizing default rates may reduce approval rate\n",
        "  - **Decision**: Balance based on business objectives and regulatory requirements\n",
        "\n",
        "**Hypothetical Example:**\n",
        "\n",
        "- **Baseline Model**: AUC = 0.75, but 0.10 AUC difference between gender groups\n",
        "- **Fairness-Adjusted Model**: AUC = 0.73, 0.02 AUC difference between groups\n",
        "- **Trade-Off**: 0.02 AUC loss for 0.08 fairness improvement\n",
        "- **Decision**: Accept trade-off if regulatory compliance requires it\n",
        "\n",
        "**Governance Framework:**\n",
        "\n",
        "1. **Model Risk Committee**:\n",
        "   - Review model for fairness before deployment\n",
        "   - Approve trade-offs between accuracy and fairness\n",
        "   - Set thresholds for acceptable disparities\n",
        "\n",
        "2. **Regular Audits**:\n",
        "   - Quarterly reviews of group-wise performance\n",
        "   - Investigate and remediate if disparities detected\n",
        "   - Document decisions and rationale\n",
        "\n",
        "3. **Transparency**:\n",
        "   - Document excluded features and rationale\n",
        "   - Report group-wise metrics to stakeholders\n",
        "   - Maintain audit trail of model decisions\n",
        "\n",
        "4. **Regulatory Compliance**:\n",
        "   - Ensure compliance with local regulations (e.g., anti-discrimination laws)\n",
        "   - Consider Sharia principles (fairness, justice)\n",
        "   - Align with bank's ethical guidelines\n",
        "\n",
        "**Sharia-Compliant Considerations:**\n",
        "\n",
        "- **Justice (Adl)**: Ensure fair treatment of all customers\n",
        "- **No Exploitation**: Avoid discriminatory practices that exploit vulnerable groups\n",
        "- **Transparency**: Clear, explainable decisions\n",
        "- **Social Responsibility**: Consider impact on community and society\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
